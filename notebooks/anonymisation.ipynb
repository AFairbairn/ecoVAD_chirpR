{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import config file and set the path\n",
    "\n",
    "In this notebook we have to manually set path as `./notebooks` is a subfolder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import soundfile\n",
    "\n",
    "import yaml\n",
    "from yaml import FullLoader\n",
    "\n",
    "# Open the config file\n",
    "with open(\"../config_inference.yaml\") as f:\n",
    "    cfg = yaml.load(f, Loader=FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_data = \"../assets/demo_data/data_to_anonymised\"\n",
    "path_json_detections = \"../assets/demo_data/detections/json/\"\n",
    "ecovad_weights_path = \"../assets/model_weights/ecoVAD_ckpt.pt\"\n",
    "path_anonymised_data = \"../assets/demo_data/anonymised_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 files to analyze\n"
     ]
    }
   ],
   "source": [
    "# List the folder with files that needs predictions\n",
    "types = ('/**/*.WAV', '/**/*.wav', '/**/*.mp3') # the tuple of file types\n",
    "audiofiles= []\n",
    "for files in types:\n",
    "    audiofiles.extend(glob.glob(path_input_data + files, recursive=True))\n",
    "\n",
    "print(\"Found {} files to analyze\".format(len(audiofiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo on using the different VAD algorithms\n",
    "\n",
    "In this first section we demonstrate how to use the VAD algorithms to detect human speech in soundscapes. We standardized the output of the VAD models so that a `.json` file is created for each input file. The demo output can be found in `./assets/demo_data/detections/json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with Pyannote\n",
    "\n",
    "Pyannote brings good performance overall even though it returns more false positive on environmental soundscape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 277/277 [00:00<00:00, 199kB/s]\n",
      "Downloading: 100%|██████████| 17.7M/17.7M [00:00<00:00, 57.6MB/s]\n",
      "Downloading: 100%|██████████| 1.98k/1.98k [00:00<00:00, 1.03MB/s]\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "from VAD_algorithms.pyannote.pyannote_predict import PyannotePredict\n",
    "\n",
    "# Load the pyannote model\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "\n",
    "# Make the prediction\n",
    "for audiofile in audiofiles:\n",
    "    out_name = audiofile.split('/')[-1].split('.')[0]\n",
    "    out_path = os.sep.join([path_json_detections, \"pyannote\",  out_name])\n",
    "    PyannotePredict(pipeline, audiofile, out_path).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with ecoVAD\n",
    "\n",
    "ecoVAD does best on environmental soundscapes but might return more false positive on urban soundcape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAD_algorithms.ecovad.ecoVAD_predict import ecoVADpredict\n",
    "\n",
    "for audiofile in audiofiles:\n",
    "    out_name = audiofile.split('/')[-1].split('.')[0]\n",
    "    out_path = os.sep.join([path_json_detections,  \"ecoVAD\", out_name])\n",
    "\n",
    "    ecoVADpredict(audiofile, \n",
    "                out_path,\n",
    "                ecovad_weights_path,\n",
    "                cfg[\"THRESHOLD\"],\n",
    "                cfg[\"USE_GPU\"]).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with webrtcVAD\n",
    "\n",
    "Webrtc VAD does not perform as well as pyannote or ecoVAD but is able to do predictions very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAD_algorithms.webrtcvad.webrtc_predict import WebrtcvadPredict\n",
    "\n",
    "for audiofile in audiofiles:\n",
    "    out_name = audiofile.split('/')[-1].split('.')[0]\n",
    "    out_path = os.sep.join([path_json_detections,  \"webrtcVAD\", out_name])\n",
    "\n",
    "    WebrtcvadPredict(audiofile, \n",
    "                    out_path,\n",
    "                    cfg[\"FRAME_LENGTH\"],\n",
    "                    cfg[\"AGGRESSIVENESS\"]).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data anonymisation\n",
    "\n",
    "Now that the VAD models have detected the human speech in the audio files, we can use the resulting `.json` to anonymise the audio. In our case, the anonymisation script consists in removing those segments where human speech has been detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 audio files with valid result file.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/app/notebooks/anonymisation.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f62656e6a616d696e2e637265746f69732f436f64652f65636f564144222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a22756e69783a2f2f2f72756e2f757365722f3231303031353136322f646f636b65722e736f636b227d7d/app/notebooks/anonymisation.ipynb#ch0000013vscode-remote?line=10'>11</a>\u001b[0m audio_name \u001b[39m=\u001b[39m afile\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f62656e6a616d696e2e637265746f69732f436f64652f65636f564144222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a22756e69783a2f2f2f72756e2f757365722f3231303031353136322f646f636b65722e736f636b227d7d/app/notebooks/anonymisation.ipynb#ch0000013vscode-remote?line=11'>12</a>\u001b[0m save_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39msep\u001b[39m.\u001b[39mjoin([path_anonymised_data, \u001b[39m\"\u001b[39m\u001b[39mANONYMISED_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m audio_name])\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f62656e6a616d696e2e637265746f69732f436f64652f65636f564144222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a22756e69783a2f2f2f72756e2f757365722f3231303031353136322f646f636b65722e736f636b227d7d/app/notebooks/anonymisation.ipynb#ch0000013vscode-remote?line=13'>14</a>\u001b[0m anonymised_arr, sr \u001b[39m=\u001b[39m audio_anonymisation(rfile, afile)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f62656e6a616d696e2e637265746f69732f436f64652f65636f564144222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a22756e69783a2f2f2f72756e2f757365722f3231303031353136322f646f636b65722e736f636b227d7d/app/notebooks/anonymisation.ipynb#ch0000013vscode-remote?line=14'>15</a>\u001b[0m soundfile\u001b[39m.\u001b[39mwrite(save_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m, anonymised_arr, sr)\n",
      "File \u001b[0;32m/app/anonymise_data.py:67\u001b[0m, in \u001b[0;36maudio_anonymisation\u001b[0;34m(json_file, audio_file)\u001b[0m\n\u001b[1;32m     <a href='file:///app/anonymise_data.py?line=63'>64</a>\u001b[0m arr, sr \u001b[39m=\u001b[39m openAudioFile(audio_file, sample_rate\u001b[39m=\u001b[39m\u001b[39m44100\u001b[39m)\n\u001b[1;32m     <a href='file:///app/anonymise_data.py?line=65'>66</a>\u001b[0m \u001b[39m# Start the anonymization loop\u001b[39;00m\n\u001b[0;32m---> <a href='file:///app/anonymise_data.py?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data[\u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m])):\n\u001b[1;32m     <a href='file:///app/anonymise_data.py?line=67'>68</a>\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mstart\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m sr)\n\u001b[1;32m     <a href='file:///app/anonymise_data.py?line=68'>69</a>\u001b[0m     e \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m][i][\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m sr)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from anonymise_data import parseFolders, audio_anonymisation\n",
    "\n",
    "parsed_folders = parseFolders(path_input_data, path_json_detections)\n",
    "\n",
    "# Anonymise the files\n",
    "for i in range(len(parsed_folders)):\n",
    "\n",
    "    afile = parsed_folders[i]['audio']\n",
    "    rfile = parsed_folders[i]['result']\n",
    "\n",
    "    audio_name = afile.split(\"/\")[-1].split(\".\")[0]\n",
    "    save_name = os.sep.join([path_anonymised_data, \"ANONYMISED_\" + audio_name])\n",
    "\n",
    "    anonymised_arr, sr = audio_anonymisation(rfile, afile)\n",
    "    soundfile.write(save_name + \".wav\", anonymised_arr, sr)\n",
    "    # Notebook result in an error but the files are anonymised \n",
    "    # The error does not occur with the main script"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
